import { NextRequest, NextResponse } from "next/server";
import { segmentChineseText } from "../../utils/chineseProcessor.server";
import WordProcessor from "../../utils/wordProcessor.enhanced";

export async function GET(request: NextRequest) {
  const { searchParams } = new URL(request.url);
  const text =
    searchParams.get("text") ||
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å¾ˆå¼€å¿ƒã€‚æˆ‘ä½åœ¨æ–°è¥¿å…°å¥¥å…‹å…°ã€‚ä½†æ˜¯ English words should be filtered out when processing Chinese text!";

  return processChineseText(text);
}

export async function POST(request: NextRequest) {
  const body = await request.json();
  const text =
    body.text ||
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å¾ˆå¼€å¿ƒã€‚æˆ‘ä½åœ¨æ–°è¥¿å…°å¥¥å…‹å…°ã€‚ä½†æ˜¯ English words should be filtered out when processing Chinese text!";

  return processChineseText(text);
}

async function processChineseText(text: string) {
  try {
    console.log("ðŸ§ª Testing Chinese text processing...");
    console.log("Input text:", text);

    // Test the Chinese processor directly
    const segments = await segmentChineseText(text);
    console.log("Segmented words:", segments);

    // Test the enhanced WordProcessor with English filtering
    const wordCloudData = await WordProcessor.processTextAsync(text, {
      locale: "zh",
    });
    console.log("Word cloud data:", wordCloudData);

    // Check for filler words that should be filtered
    const fillerWordsToCheck = [
      "ä¸€ä¸ª",
      "è¿™ä¸ª",
      "é‚£ä¸ª",
      "ä»€ä¹ˆ",
      "å› ä¸º",
      "çŽ°åœ¨",
      "ä¸€ä¸‹",
      "ä¹Ÿæ˜¯",
      "éƒ½æ˜¯",
      "ç„¶åŽ",
      "ä½†æ˜¯",
      "æ‰€ä»¥",
      "è™½ç„¶",
      "å¦‚æžœ",
      "æˆ–è€…",
      "å·²ç»",
      "è¿˜æ˜¯",
      "åº”è¯¥",
      "å¯èƒ½",
      "å½“ç„¶",
      "å…¶å®ž",
      "æ¯”å¦‚",
      "å°±æ˜¯",
      "è€Œä¸”",
      "ä¸è¿‡",
      "å¯æ˜¯",
      "è§‰å¾—",
      "æ„Ÿè§‰",
      "çŸ¥é“",
      "è®¤ä¸º",
      "å¥½åƒ",
      "ä¼¼ä¹Ž",
      "å¤§æ¦‚",
      "ä¼°è®¡",
      "æˆ–è®¸",
      "ä¹Ÿè®¸",
    ];

    const fillerWordsFound = segments.filter((word) =>
      fillerWordsToCheck.includes(word)
    );

    // Check for basic stop words
    const stopWordsFound = segments.filter((word) =>
      ["çš„", "äº†", "æˆ‘ä»¬", "éƒ½", "å¾ˆ"].includes(word)
    );

    const repetitiveFound = segments.filter(
      (word) => /^(.)\1+$/.test(word) || word === "å“ˆå“ˆå“ˆ" || word === "å˜»å˜»å˜»"
    );

    // Test if we have proper individual words (not long phrases)
    const properWords = segments.filter(
      (word) =>
        word.length >= 2 &&
        word.length <= 4 &&
        !fillerWordsToCheck.includes(word)
    );
    const longPhrases = segments.filter((word) => word.length > 6);

    return NextResponse.json({
      input: text,
      segments: segments,
      wordCloudData: wordCloudData,
      totalWords: segments.length,
      fillerWordsFound: fillerWordsFound,
      stopWordsFound: stopWordsFound,
      repetitiveFound: repetitiveFound,
      properWords: properWords,
      longPhrases: longPhrases,
      englishWordsInWordCloud: wordCloudData.filter((item) =>
        /^[a-zA-Z\s]+$/.test(item.text)
      ),
      status: segments.length > 0 ? "SUCCESS" : "NO_WORDS",
      validation: {
        hasFillerWords: fillerWordsFound.length > 0,
        hasStopWords: stopWordsFound.length > 0,
        hasRepetitive: repetitiveFound.length > 0,
        properSegmentation: segments.some((word) => word.length >= 2),
        hasProperWords: properWords.length > 0,
        hasLongPhrases: longPhrases.length > 0,
        englishFilteringWorking:
          wordCloudData.filter((item) => /^[a-zA-Z\s]+$/.test(item.text))
            .length === 0,
      },
    });
  } catch (error) {
    console.error("Error testing Chinese processing:", error);
    return NextResponse.json(
      {
        error: error instanceof Error ? error.message : String(error),
        status: "ERROR",
      },
      { status: 500 }
    );
  }
}
